{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Project",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Here, we'll import some basic libraries, enable CUDA, and mount this to your account."
      ],
      "metadata": {
        "id": "LAQDrHfrwGdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "EgTNKdOVstR5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GS0yuGl0mHQ"
      },
      "source": [
        "print('Version', torch.__version__)\n",
        "print('CUDA enabled:', torch.cuda.is_available())\n",
        "  \n",
        "# Running this should then print out:\n",
        "# Version 1.7.0+cu101 (or something like this)\n",
        "# CUDA enabled: True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_NLDJL-vOTi"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls /gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLVJPc_90vsB"
      },
      "source": [
        "import os\n",
        "BASE_PATH = '/gdrive/My Drive/colab_files/project'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "DATA_PATH = '/content/'\n",
        "\n",
        "if not os.path.exists(os.path.join(DATA_PATH, 'harry_potter.txt')):\n",
        "    os.chdir(BASE_PATH)\n",
        "    !wget https://courses.cs.washington.edu/courses/cse599g1/19au/files/homework3.tar.gz\n",
        "    !tar -zxvf homework3.tar.gz\n",
        "    !rm homework3.tar.gz\n",
        "    !cp pt_util.py /content\n",
        "os.chdir('/content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Data\n",
        "\n",
        "The dataset we're using is the Jena Climate dataset from the Max Planck Institute for Biogeochemistry."
      ],
      "metadata": {
        "id": "4K3EUWaVNfHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "import torchtext.utils as utils\n",
        "import pt_util\n",
        "\n",
        "url = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\" \n",
        "\n",
        "path = utils.download_from_url(url)\n",
        "\n",
        "file = ZipFile(path)\n",
        "file.extractall()\n",
        "\n",
        "csv_path = \"jena_climate_2009_2016.csv\""
      ],
      "metadata": {
        "id": "UIPl82BRtwa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "TRAIN_SPLIT = 0.7\n",
        "EVAL_SPLIT = 0.1\n",
        "\n",
        "def prepare_data(data_path):\n",
        "  df = pd.read_csv(data_path)\n",
        "\n",
        "  # Each sample is in ten minute intervals, so select one every hour for hour intervals\n",
        "  df = df[5::6] \n",
        "\n",
        "  wv = df['wv (m/s)']\n",
        "  df['wv (m/s)'] = wv.where(wv >= 0, 0)\n",
        "\n",
        "  mv = df['max. wv (m/s)']\n",
        "  df['max. wv (m/s)'] = mv.where(mv >= 0, 0)\n",
        "\n",
        "  wv = df.pop('wv (m/s)')\n",
        "  mv = df.pop('max. wv (m/s)')\n",
        "  wd = df.pop('wd (deg)')\n",
        "\n",
        "  wr = wd * np.pi / 180\n",
        "  df['wx (m/s)'] = wv * np.cos(wr)\n",
        "  df['wy (m/s)'] = wv * np.sin(wr)\n",
        "\n",
        "  df['max. wx (m/s)'] = mv * np.cos(wr)\n",
        "  df['max. wy(m/s)'] = mv * np.sin(wr)\n",
        "\n",
        "  date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
        "\n",
        "  timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
        "  day = 24 * 60 * 60\n",
        "  year = 365.2425 * day\n",
        "\n",
        "  df['Day x'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
        "  df['Day y'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
        "  df['Year x'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
        "  df['Year y'] = np.sin(timestamp_s * (2 * np.pi /year))\n",
        "\n",
        "  train_df, eval_df, test_df = normalize(df)\n",
        "  pickle.dump({'data': train_df}, open(DATA_PATH + 'jena_train_climate_data.pkl', 'wb'))\n",
        "  pickle.dump({'data':eval_df}, open(DATA_PATH + 'jena_eval_climate_data.pkl', 'wb'))\n",
        "  pickle.dump({'data': test_df}, open(DATA_PATH + 'jena_test_climate_data.pkl', 'wb'))\n",
        "\n",
        "def normalize(data):\n",
        "  train_end = int(TRAIN_SPLIT * data.shape[0])\n",
        "  eval_end = train_end + int(EVAL_SPLIT * data.shape[0])\n",
        "\n",
        "  train_df = data[:train_end]\n",
        "  eval_df = data[train_end: eval_end]\n",
        "  test_df = data[eval_end:]\n",
        "\n",
        "  train_mean = train_df.mean()\n",
        "  train_std = train_df.std()\n",
        "\n",
        "  train_df = (train_df - train_mean) / train_std\n",
        "  eval_df = (eval_df - train_mean) / train_std\n",
        "  test_df =  (test_df - train_mean) / train_std\n",
        "  return train_df, eval_df, test_df\n",
        "\n",
        "prepare_data(csv_path)\n",
        "  "
      ],
      "metadata": {
        "id": "arLW8apX1GYf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the dataset\n",
        "\n",
        "The data has been cleaned, but now it needs to be turned into a set of inputs and a set of labels. "
      ],
      "metadata": {
        "id": "vo0ot0-2w0hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make dataset\n",
        "\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class WeatherDataset(Dataset):\n",
        "  def __init__(self, data_file, offset, input_length, output_labels=None):\n",
        "    super(WeatherDataset, self).__init__()\n",
        "\n",
        "    with open(data_file, 'rb') as data_pkl:\n",
        "      dataset = pickle.load(data_pkl)\n",
        "    self.df = dataset['data']\n",
        "\n",
        "    self.input_length = input_length\n",
        "    self.offset = offset\n",
        "\n",
        "    if output_labels is None:\n",
        "      output_labels = list(self.df.columns)\n",
        "    inputs = []\n",
        "    labels = []\n",
        "\n",
        "    for i in range(self.df.shape[0] - offset - input_length):\n",
        "      x = self.df[i : i + input_length]\n",
        "      y = self.df.iloc[i + offset : i + input_length + offset]\n",
        "      inputs.append(x)\n",
        "      labels.append(y[output_labels])\n",
        "    self.inputs = np.array(inputs)\n",
        "    self.labels = np.array(labels)\n",
        "\n",
        "    \n",
        "  def __len__(self):\n",
        "    return self.inputs.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return torch.from_numpy(self.inputs[idx]).float(), torch.from_numpy(self.labels[idx]).float()\n"
      ],
      "metadata": {
        "id": "_ZIIouFwGOO4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BaseModel"
      ],
      "metadata": {
        "id": "1KezKYUkVasg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BaseModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BaseModel, self).__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = np.squeeze(x)\n",
        "    return x[:,1].flatten()\n",
        "\n",
        "  def inference(self, x, temperature=1):\n",
        "    x = self.forward(x)\n",
        "    x = x / max(temperature, 1e-20)\n",
        "    return x\n",
        "\n",
        "  def loss(self, prediction, label, reduction='mean'):\n",
        "    return F.mse_loss(prediction,label, reduction=reduction)\n",
        "\n",
        "  def evaluate(self, x):\n",
        "    return self.forward(x)"
      ],
      "metadata": {
        "id": "D_7xs5RXVL7I"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(model, device, train_loader, epoch, log_interval):\n",
        "  model.train()\n",
        "  losses = []\n",
        "\n",
        "  for batch_idx, (data, label) in enumerate(tqdm.tqdm(train_loader)):\n",
        "    data = data.to(device)\n",
        "    label = label.to(device)\n",
        "    output = model(data)\n",
        "    loss = model.loss(output, label)\n",
        "    losses.append(loss.item())\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset), \n",
        "          100. * batch_idx * len(train_loader), loss.item()))\n",
        "  return np.mean(losses)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "\n",
        "  for batch_idx, (data, label) in enumerate(tqdm.tqdm(test_loader)):\n",
        "    data = data.to(device)\n",
        "    label = label.to(device)\n",
        "    output = model(data)\n",
        "    test_loss += model.loss(output, label).item()\n",
        "\n",
        "  test_loss /= len(test_loader)\n",
        "  print('\\nTest set: Average loss: {:.4f} \\n'.format(test_loss))\n",
        "  return test_loss\n",
        "    \n",
        "       \n"
      ],
      "metadata": {
        "id": "jdF1RfXoXvkO"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhGDSK0ZwBEt"
      },
      "source": [
        "def main():\n",
        "    INPUT_LENGTH = 12\n",
        "    OFFSET = 12\n",
        "    BATCH_SIZE = 128\n",
        "    INPUT_SIZE = 19\n",
        "    HIDDEN_SIZE = 512\n",
        "    TEST_BATCH_SIZE = 128\n",
        "    EPOCHS = 10\n",
        "    OUTPUT_SIZE = 1 \n",
        "    LEARNING_RATE = 0.002\n",
        "    WEIGHT_DECAY = 0.0005\n",
        "    USE_CUDA = True\n",
        "    PRINT_INTERVAL = 10\n",
        "    LOG_PATH = DATA_PATH + 'logs/log.pkl'\n",
        "\n",
        "\n",
        "    data_train = WeatherDataset(DATA_PATH + 'jena_train_climate_data.pkl', OFFSET, INPUT_LENGTH, \n",
        "                                output_labels=['T (degC)'])\n",
        "    data_test = WeatherDataset(DATA_PATH + 'jena_test_climate_data.pkl', OFFSET, INPUT_LENGTH,\n",
        "                               output_labels=['T (degC)'])\n",
        "\n",
        "    use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print('Using device', device)\n",
        "    import multiprocessing\n",
        "    num_workers = multiprocessing.cpu_count()\n",
        "    print('num workers:', num_workers)\n",
        "\n",
        "    kwargs = {'num_workers': num_workers,\n",
        "              'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                              shuffle=True, **kwargs)\n",
        "\n",
        "    model = BaseModel().to(device)\n",
        "\n",
        "\n",
        "    start_epoch = 0\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    test_loss = test(model, device, test_loader)\n",
        "\n",
        "    test_losses.append((start_epoch, test_loss))\n",
        "\n",
        "    try:\n",
        "        for epoch in range(start_epoch, EPOCHS + 1):\n",
        "            lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "            train_loss = train(model, device, train_loader, epoch, PRINT_INTERVAL)\n",
        "            test_loss = test(model, device, test_loader)\n",
        "            train_losses.append((epoch, train_loss))\n",
        "            test_losses.append((epoch, test_loss))\n",
        "    except KeyboardInterrupt as ke:\n",
        "        print('Interrupted')\n",
        "    except:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        fig, (axis1, axis2) = plt.subplots(2)\n",
        "        ep, val = zip(*train_losses)\n",
        "        axis1.plot(ep, val)\n",
        "        axis1.set(title=\"Train loss\", xlabel=\"Epoch\", ylabel=\"Error\")\n",
        "        ep, val = zip(*test_losses)\n",
        "        axis2.plot(ep, val)\n",
        "        axis2.set(title=\"Test loss\", xlabel=\"Epoch\", ylabel=\"Error\")\n",
        "        fig.tight_layout()\n",
        "        plt.show()\n",
        "        return model, device\n",
        "\n",
        "base_model, device = main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "CRVOWmSCxVRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LSTMNet(nn.Module):\n",
        "  def __init__(self, hidden_dim, input_size, output_size, num_layers=4):\n",
        "    super(LSTMNet, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_layers = num_layers\n",
        "    self.lstm = nn.LSTM(input_size, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "    self.decoder = nn.Linear(hidden_dim, output_size) \n",
        "    self.best_loss = 2**32\n",
        "\n",
        "  def forward(self, x, hidden=None):\n",
        "    x, hidden = self.lstm(x, hidden)\n",
        "    x = self.decoder(x)\n",
        "    return x, hidden\n",
        "\n",
        "  def init_hidden(self, num_layers, batch_size):\n",
        "    hidden = torch.zeros((num_layers, batch_size, self.hidden_dim)).cuda()\n",
        "    cell = torch.zeros((num_layers, batch_size, self.hidden_dim)).cuda()\n",
        "    return hidden, cell\n",
        "\n",
        "  def inference(self, x, hidden_state=None, temperature=1):\n",
        "    x, hidden = self.forward(x, hidden_state)\n",
        "    x = x / max(temperature, 1e-20)\n",
        "    return x, hidden\n",
        "\n",
        "  def loss(self, prediction, label, reduction='mean'):\n",
        "    loss_val = F.mse_loss(prediction, label, reduction=reduction)\n",
        "    return loss_val\n",
        "  \n",
        "  def evaluate(self, x):\n",
        "    hidden = self.init_hidden(self.num_layers, x.shape[0])\n",
        "    pred, _ = self.forward(x, hidden)\n",
        "    return pred\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qZr9UgtcufLa"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x1mmS5uy0ro"
      },
      "source": [
        "import tqdm\n",
        "import math\n",
        "\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train(model, device, optimizer, train_loader, lr, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx, (data, label) in enumerate(tqdm.tqdm(train_loader)):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        hidden = model.init_hidden(model.num_layers, data.shape[0])\n",
        "        if hidden is not None:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "        optimizer.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            hidden = model.init_hidden(model.num_layers, data.shape[0])\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output, hidden = model(data, hidden)\n",
        "            loss = model.loss(output, label, reduction='mean').item()\n",
        "            test_loss += loss\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f} \\n'.format(\n",
        "        test_loss))\n",
        "    return test_loss"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nuWelSd1KMZ"
      },
      "source": [
        "def main():\n",
        "    INPUT_LENGTH = 12\n",
        "    OFFSET = 12\n",
        "    BATCH_SIZE = 128\n",
        "    INPUT_SIZE = 19\n",
        "    HIDDEN_SIZE = 512\n",
        "    TEST_BATCH_SIZE = 128\n",
        "    EPOCHS = 10\n",
        "    OUTPUT_SIZE = 1\n",
        "    LEARNING_RATE = 0.002\n",
        "    WEIGHT_DECAY = 0.0005\n",
        "    USE_CUDA = True\n",
        "    PRINT_INTERVAL = 10\n",
        "    LOG_PATH = DATA_PATH + 'logs/log.pkl'\n",
        "\n",
        "\n",
        "    data_train = WeatherDataset(DATA_PATH + 'jena_train_climate_data.pkl', OFFSET, INPUT_LENGTH, \n",
        "                                output_labels=['T (degC)'])\n",
        "    data_test = WeatherDataset(DATA_PATH + 'jena_test_climate_data.pkl', OFFSET, INPUT_LENGTH,\n",
        "                               output_labels=['T (degC)'])\n",
        "\n",
        "    use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print('Using device', device)\n",
        "    import multiprocessing\n",
        "    num_workers = multiprocessing.cpu_count()\n",
        "    print('num workers:', num_workers)\n",
        "\n",
        "    kwargs = {'num_workers': num_workers,\n",
        "              'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                              shuffle=True, **kwargs)\n",
        "\n",
        "    model = LSTMNet(HIDDEN_SIZE, INPUT_SIZE, OUTPUT_SIZE).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    start_epoch = 0\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    test_loss = test(model, device, test_loader)\n",
        "\n",
        "    test_losses.append((start_epoch, test_loss))\n",
        "\n",
        "    try:\n",
        "        for epoch in range(start_epoch, EPOCHS + 1):\n",
        "            lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "            train_loss = train(model, device, optimizer, train_loader, lr, epoch, PRINT_INTERVAL)\n",
        "            test_loss = test(model, device, test_loader)\n",
        "            train_losses.append((epoch, train_loss))\n",
        "            test_losses.append((epoch, test_loss))\n",
        "    except KeyboardInterrupt as ke:\n",
        "        print('Interrupted')\n",
        "    except:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        fig, (axis1, axis2) = plt.subplots(2)\n",
        "        ep, val = zip(*train_losses)\n",
        "        axis1.plot(ep, val)\n",
        "        axis1.set(title=\"Train loss\", xlabel=\"Epoch\", ylabel=\"Error\")\n",
        "        ep, val = zip(*test_losses)\n",
        "        axis2.plot(ep, val)\n",
        "        axis2.set(title=\"Test loss\", xlabel=\"Epoch\", ylabel=\"Error\")\n",
        "        fig.tight_layout()\n",
        "        plt.show()\n",
        "        return model, device\n",
        "\n",
        "lstm_model, device = main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU"
      ],
      "metadata": {
        "id": "UfuHfbKNnVXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GRUNet(nn.Module):\n",
        "  def __init__(self, input_size, feature_size, num_layers=4):\n",
        "    super(GRUNet, self).__init__()\n",
        "\n",
        "    self.feature_size = feature_size\n",
        "    self.input_size = input_size\n",
        "    self.num_layers = num_layers\n",
        "    self.gru = nn.GRU(self.input_size, self.feature_size, num_layers=self.num_layers, \n",
        "                      batch_first=True)\n",
        "    self.decoder = nn.Linear(self.feature_size, 1) \n",
        "    self.best_loss = 2**32\n",
        "\n",
        "  def init_hidden(self, num_layers, batch_size):\n",
        "    return torch.zeros((num_layers, batch_size, self.feature_size)).cuda()\n",
        "\n",
        "  def forward(self, x, hidden_state=None):\n",
        "    x, hidden = self.gru(x, hidden_state)\n",
        "    x = self.decoder(x)\n",
        "    return x, hidden\n",
        "\n",
        "  def inference(self, x,  temperature=1):\n",
        "    x = x.view(-1, 1)\n",
        "    x = self.forward(x)\n",
        "    x = x.view(-1, 1)\n",
        "    x = x / max(temperature, 1e-20)\n",
        "    return x\n",
        "\n",
        "  def loss(self, prediction, label, reduction='mean'):\n",
        "    loss_val = F.mse_loss(np.squeeze(prediction), np.squeeze(label), reduction=reduction)\n",
        "    return loss_val\n",
        "\n",
        "  def evaluate(self, x):\n",
        "    hidden = self.init_hidden(self.num_layers, x.shape[0])\n",
        "    pred, _ = self.forward(x, hidden)\n",
        "    return pred\n",
        "\n"
      ],
      "metadata": {
        "id": "yRj5YV84BpTu"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oybKOcMCqrmh"
      },
      "source": [
        "\n",
        "import tqdm\n",
        "import math\n",
        "\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train(model, device, optimizer, train_loader, lr, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, label) in enumerate(tqdm.tqdm(train_loader)):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        hidden = model.init_hidden(4, data.shape[0])\n",
        "        if hidden is not None:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "        optimizer.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            hidden = model.init_hidden(4, data.shape[0])\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output, hidden = model(data, hidden)\n",
        "            loss = model.loss(output, label, reduction='mean').item()\n",
        "            test_loss += loss\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f} \\n'.format(\n",
        "        test_loss))\n",
        "    return test_loss"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POtQSySrpfjy"
      },
      "source": [
        "\n",
        "def main():\n",
        "    INPUT_LENGTH = 12\n",
        "    OFFSET = 12\n",
        "    BATCH_SIZE = 128\n",
        "    INPUT_SIZE = 19\n",
        "    FEATURE_SIZE = 512\n",
        "    TEST_BATCH_SIZE = 128\n",
        "    EPOCHS = 10\n",
        "    LEARNING_RATE = 0.002\n",
        "    WEIGHT_DECAY = 0.0005\n",
        "    USE_CUDA = True\n",
        "    PRINT_INTERVAL = 10\n",
        "\n",
        "\n",
        "    data_train = WeatherDataset(DATA_PATH + 'jena_train_climate_data.pkl', OFFSET, INPUT_LENGTH,\n",
        "                                output_labels=['T (degC)'])\n",
        "    data_test = WeatherDataset(DATA_PATH + 'jena_test_climate_data.pkl', OFFSET, INPUT_LENGTH,\n",
        "                               output_labels=['T (degC)'])\n",
        "\n",
        "    use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print('Using device', device)\n",
        "    import multiprocessing\n",
        "    num_workers = multiprocessing.cpu_count()\n",
        "    print('num workers:', num_workers)\n",
        "\n",
        "    kwargs = {'num_workers': num_workers,\n",
        "              'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                              shuffle=True, **kwargs)\n",
        "\n",
        "    model = GRUNet(INPUT_SIZE, FEATURE_SIZE).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    start_epoch = 0\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    test_loss = test(model, device, test_loader)\n",
        "\n",
        "    test_losses.append((start_epoch, test_loss))\n",
        "\n",
        "    try:\n",
        "        for epoch in range(start_epoch, EPOCHS + 1):\n",
        "            lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "            train_loss = train(model, device, optimizer, train_loader, lr, epoch, PRINT_INTERVAL)\n",
        "            test_loss = test(model, device, test_loader)\n",
        "            train_losses.append((epoch, train_loss))\n",
        "            test_losses.append((epoch, test_loss))\n",
        "    except KeyboardInterrupt as ke:\n",
        "        print('Interrupted')\n",
        "    except:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        fig, (axis1, axis2) = plt.subplots(2)\n",
        "        ep, val = zip(*train_losses)\n",
        "        axis1.plot(ep, val)\n",
        "        axis1.set(title=\"Train loss\", xlabel=\"Epoch\", ylabel=\"Error\")\n",
        "        ep, val = zip(*test_losses)\n",
        "        axis2.plot(ep, val)\n",
        "        axis2.set(title=\"Test loss\", xlabel=\"Epoch\", ylabel=\"Error\")\n",
        "        fig.tight_layout()\n",
        "        fig.show()\n",
        "\n",
        "        return model, device\n",
        "\n",
        "gru_model, device = main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer "
      ],
      "metadata": {
        "id": "yf04vY1Fnus_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerNet(nn.Module):\n",
        "  def __init__(self, input_size, num_layers=4, dropout=0.1):\n",
        "    super(TransformerNet, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.input_size, nhead=1, dropout=dropout)\n",
        "    self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=self.num_layers)\n",
        "    self.decoder = nn.Linear(self.input_size, 1) \n",
        "    self.best_loss = 2**32\n",
        "\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = self.transformer(x, mask)\n",
        "    x = self.decoder(x)\n",
        "    return x\n",
        "\n",
        "  def inference(self, x, mask, temperature=1):\n",
        "    x = x.view(-1, 1)\n",
        "    x = self.forward(x, mask)\n",
        "    x = x.view(-1, 1)\n",
        "    x = x / max(temperature, 1e-20)\n",
        "    return x\n",
        "\n",
        "  def loss(self, prediction, label, reduction='mean'):\n",
        "    loss_val = F.mse_loss(np.squeeze(prediction), np.squeeze(label), reduction=reduction)\n",
        "    return loss_val\n",
        "\n",
        "  def evaluate(self, x):\n",
        "    mask = nn.Transformer.generate_square_subsequent_mask(x.shape[0]).cuda()\n",
        "    return self.forward(x, mask)\n"
      ],
      "metadata": {
        "id": "_sv-_0KWECTv"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Wbx-xiFoET5"
      },
      "source": [
        "import tqdm\n",
        "import math\n",
        "\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train(model, device, optimizer, train_loader, lr, epoch, log_interval, batch_size):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    mask = nn.Transformer.generate_square_subsequent_mask(batch_size).to(device)\n",
        "    for batch_idx, (data, label) in enumerate(tqdm.tqdm(train_loader)):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        if data.shape[0] != batch_size:\n",
        "          mask = mask[:data.shape[0],:data.shape[0]]\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data, mask)\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, batch_size):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    mask = nn.Transformer.generate_square_subsequent_mask(batch_size).to(device)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            if data.shape[0] != batch_size:\n",
        "              mask = mask[:data.shape[0], :data.shape[0]]\n",
        "            output = model(data, mask)\n",
        "            loss = model.loss(output, label, reduction='mean').item()\n",
        "            test_loss += loss\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f} \\n'.format(\n",
        "        test_loss))\n",
        "    return test_loss"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDyzAr1SpEp0"
      },
      "source": [
        "def main():\n",
        "    INPUT_LENGTH = 12\n",
        "    OFFSET = 12\n",
        "    NUM_LAYERS = 4\n",
        "    BATCH_SIZE = 128\n",
        "    INPUT_SIZE = 19\n",
        "    TEST_BATCH_SIZE = 128\n",
        "    EPOCHS = 10\n",
        "    LEARNING_RATE = 0.002\n",
        "    WEIGHT_DECAY = 0.0005\n",
        "    USE_CUDA = True\n",
        "    PRINT_INTERVAL = 10\n",
        "\n",
        "    data_train = WeatherDataset(DATA_PATH + 'jena_train_climate_data.pkl', OFFSET, INPUT_LENGTH,\n",
        "                                output_labels=['T (degC)'])\n",
        "    data_test = WeatherDataset(DATA_PATH + 'jena_test_climate_data.pkl', OFFSET, INPUT_LENGTH,\n",
        "                                output_labels=['T (degC)'])\n",
        "\n",
        "    use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print('Using device', device)\n",
        "    import multiprocessing\n",
        "    num_workers = multiprocessing.cpu_count()\n",
        "    print('num workers:', num_workers)\n",
        "\n",
        "    kwargs = {'num_workers': num_workers,\n",
        "              'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                              shuffle=True, **kwargs)\n",
        "\n",
        "    model = TransformerNet(INPUT_SIZE, num_layers=NUM_LAYERS).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    start_epoch = 0\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    test_loss = test(model, device, test_loader, TEST_BATCH_SIZE)\n",
        "\n",
        "    test_losses.append((start_epoch, test_loss))\n",
        "\n",
        "    try:\n",
        "        for epoch in range(start_epoch, EPOCHS + 1):\n",
        "            lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "            train_loss = train(model, device, optimizer, train_loader, lr, \n",
        "                               epoch, PRINT_INTERVAL, BATCH_SIZE)\n",
        "            test_loss = test(model, device, test_loader, TEST_BATCH_SIZE)\n",
        "            train_losses.append((epoch, train_loss))\n",
        "            test_losses.append((epoch, test_loss))\n",
        "    except KeyboardInterrupt as ke:\n",
        "        print('Interrupted')\n",
        "    except:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        fig, (axis1, axis2) = plt.subplots(2)\n",
        "        ep, val = zip(*train_losses)\n",
        "        axis1.plot(ep, val)\n",
        "        axis1.set(title=\"Train loss\", xlabel=\"Epoch\", ylabel=\"Error\")\n",
        "        ep, val = zip(*test_losses)\n",
        "        axis2.plot(ep, val)\n",
        "        axis2.set(title=\"Test loss\", xlabel=\"Epoch\", ylabel=\"Error\")\n",
        "        fig.tight_layout()\n",
        "        fig.show()\n",
        "        return model, device\n",
        "\n",
        "transformer_model, device = main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Net"
      ],
      "metadata": {
        "id": "VDljntJbLAEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.linear_stack = nn.Sequential(\n",
        "        nn.Linear(input_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear_stack(x)\n",
        "\n",
        "  def loss(self, prediction, label, reduction='mean'):\n",
        "    return F.mse_loss(prediction, label, reduction)\n",
        "\n",
        "  def evaluate(self, x):\n",
        "    return self.forward(x)\n"
      ],
      "metadata": {
        "id": "a6pxM2lDJsbj"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At53fnMjLLfg"
      },
      "source": [
        "import tqdm\n",
        "import math\n",
        "\n",
        "\n",
        "def train(model, device, optimizer, train_loader, lr, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx, (data, label) in enumerate(tqdm.tqdm(train_loader)):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = model.loss(output, label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            output = model(data)\n",
        "            loss = model.loss(output, label, reduction='mean').item()\n",
        "            test_loss += loss\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f} \\n'.format(\n",
        "        test_loss))\n",
        "    return test_loss"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtoFcDfNLwZu"
      },
      "source": [
        "\n",
        "def main():\n",
        "    INPUT_LENGTH = 12\n",
        "    OFFSET = 12\n",
        "    BATCH_SIZE = 128\n",
        "    INPUT_SIZE = 19\n",
        "    HIDDEN_SIZE = 512\n",
        "    TEST_BATCH_SIZE = 128\n",
        "    EPOCHS = 10\n",
        "    LEARNING_RATE = 0.002   \n",
        "    WEIGHT_DECAY = 0.0005\n",
        "    USE_CUDA = True\n",
        "    PRINT_INTERVAL = 10\n",
        "\n",
        "    data_train = WeatherDataset(DATA_PATH + 'jena_train_climate_data.pkl', OFFSET, INPUT_LENGTH,\n",
        "                                output_labels=['T (degC)'])\n",
        "    data_test = WeatherDataset(DATA_PATH + 'jena_test_climate_data.pkl', OFFSET, INPUT_LENGTH,\n",
        "                                output_labels=['T (degC)'])\n",
        "\n",
        "    use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print('Using device', device)\n",
        "    import multiprocessing\n",
        "    num_workers = multiprocessing.cpu_count()\n",
        "    print('num workers:', num_workers)\n",
        "\n",
        "    kwargs = {'num_workers': num_workers,\n",
        "              'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\n",
        "                                              shuffle=True, **kwargs)\n",
        "\n",
        "    model = NeuralNet(INPUT_SIZE, HIDDEN_SIZE).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    start_epoch = 0\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    test_loss = test(model, device, test_loader)\n",
        "\n",
        "    test_losses.append((start_epoch, test_loss))\n",
        "\n",
        "    try:\n",
        "        for epoch in range(start_epoch, EPOCHS + 1):\n",
        "            lr = LEARNING_RATE * np.power(0.25, (int(epoch / 6)))\n",
        "            train_loss = train(model, device, optimizer, train_loader, lr, \n",
        "                               epoch, PRINT_INTERVAL)\n",
        "            test_loss = test(model, device, test_loader)\n",
        "            train_losses.append((epoch, train_loss))\n",
        "            test_losses.append((epoch, test_loss))\n",
        "    except KeyboardInterrupt as ke:\n",
        "        print('Interrupted')\n",
        "    except:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        fig, (axis1, axis2) = plt.subplots(2)\n",
        "        ep, val = zip(*train_losses)\n",
        "        axis1.plot(ep, val)\n",
        "        axis1.set(title=\"Train loss\", xlabel=\"Epoch\", ylabel=\"Error\")\n",
        "        ep, val = zip(*test_losses)\n",
        "        axis2.plot(ep, val)\n",
        "        axis2.set(title=\"Test loss\", xlabel=\"Epoch\", ylabel=\"Error\")\n",
        "        fig.tight_layout()\n",
        "        fig.show()\n",
        "        return model, device\n",
        "\n",
        "neural_model, device = main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Models"
      ],
      "metadata": {
        "id": "iK7La1kEkWzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_offset = 12\n",
        "eval_input = 12\n",
        "USE_CUDA = True\n",
        "data_eval = WeatherDataset(DATA_PATH + 'jena_eval_climate_data.pkl', eval_offset, eval_input,\n",
        "                         output_labels=['T (degC)'])\n",
        "sample_size = 1\n",
        "\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "print('num workers:', num_workers)\n",
        "\n",
        "kwargs = {'num_workers': num_workers,\n",
        "              'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "eval_loader = torch.utils.data.DataLoader(data_eval, batch_size=sample_size,\n",
        "                                          shuffle=True, **kwargs)\n",
        "\n",
        "\n",
        "def make_plot_data(input, target, input_labels, pred, input_length, offset):\n",
        "  pred_start = input_length + offset\n",
        "  pred_end = pred_start + input_length\n",
        "\n",
        "  pred_x = np.arange(pred_start, pred_end)\n",
        "  combined_x = np.append(np.arange(input_length), np.arange(pred_start, pred_end)).flatten()\n",
        "  combined_y = np.append(input_labels.cpu(), target.cpu()).flatten()\n",
        "\n",
        "  return pred_x, combined_x, combined_y\n",
        "\n",
        "\n",
        "def plot_data(pred_x, pred_y, target, combined_x, \n",
        "              combined_y, title, xlabel, ylabel):\n",
        "  plt.scatter(pred_x, target.cpu(), marker='o', label=\"Actual\")\n",
        "  plt.scatter(pred_x, pred_y.cpu(), marker='X', label=\"Prediction\")\n",
        "  plt.plot(combined_x, combined_y)\n",
        "  plt.title(title)\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def evaluate_models(models, device, input_length, offset):\n",
        "  data, label = next(iter(eval_loader))\n",
        "\n",
        "  for i in range(data.shape[0]):\n",
        "    input = data[i].to(device)\n",
        "    target = label[i].to(device)\n",
        "    temp_y = input[:, 1]\n",
        "    input = input.view((1, input.shape[0], input.shape[1]))\n",
        "    target = target.view((1, target.shape[0], target.shape[1]))\n",
        "    with torch.no_grad():\n",
        "      for key in models:\n",
        "        model = models[key]\n",
        "        pred = model.evaluate(input)\n",
        "        print('Sample {}:\\n'.format(i))\n",
        "        pred_x, combined_x, combined_y = make_plot_data(input, target,\n",
        "                                                      temp_y, pred, input_length, offset)\n",
        "        plot_data(pred_x, pred, target, combined_x, combined_y, \"Weather Prediction \" + key,\n",
        "                  \"Hour\", \"Temperature (degC)\")\n",
        "        print(key + \" Results\")\n",
        "        print('Eval loss: {:.4f} \\n'.format(model.loss(pred, target)))\n",
        "    \n",
        "models = {\"Base\": BaseModel(), \"LSTM\": lstm_model, \"GRU\": gru_model, \"Transformer\": transformer_model, \n",
        "          \"Neural Net\": neural_model}\n",
        "evaluate_models(models, device, eval_input, eval_offset)\n",
        " \n"
      ],
      "metadata": {
        "id": "bCS4j8GwGus-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}